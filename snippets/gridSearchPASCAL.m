function [errors, aps] = gridSearchPASCAL(mode, varargin)
% GRIDSEARCHPASCAL   Entry point for the code base. Not really grid search or 
% PASCAL, but conduct experiments by adjusting a number of hyperparameters and
% settings. The experiment is specified by MODE string.
% 
%   Input:
%     MODE string for the name of the experiment.
%   Output:
%     ERRORS report the error rates no matter what experiment it is.
%     APS report the average precision no matter what experiment it is.
%   Options:
%     IRUN specifies the run_ID for the multiple runs
% 
% Authors: Zhizhong Li
% 
% See the COPYING file.

dbstop if error;
opts.pause_on_done = false;
opts.irun = 1;
opts = vl_argparse(opts, varargin);

flag_generate_resultmat = false; % [ HACK ] shortcut for aggregating some of the existing experiment results


methods = [];
fakesave = '@lrchange';
partial_randseeds = [ 987164561 987264561 987364561 ]; % fixed seed for different iruns (generated by random typing)

% -------------------------------------------------------------------------
% Experiment selection
%
% This section works by generating a multi-dimensional grid for hyperparameters
% and settings such as learning rate, old-task-new-task pair, and baseline or 
% method used, and evaluating them on each grid point.
% Most grid dimensions are enumerations (e.g. the method used), whose actual 
% values are looked up in a list after generating the grid. Others are used as 
% is. See the next section for the meaning of each dimension.
% Note that not all these experiment are reported in the paper due to space
% constraints and level of interest.
% -------------------------------------------------------------------------

switch mode
    case 'TRDATA'
        % varying the data subsampled for each new task
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1,3,4,5],[2,1,4,5,8],[1],[1],[2],[2],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case 'BRANCH' 
        % different # of task-specific layers. Done on Places2 -> VOC
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1,2,4],[1],[2,1,4,5,8],[1],[1],[2],[ 2 ],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case 'BRANCHVOC' 
        % different # of task-specific layers. Done for only _lockkeep_TEMP2 (a.k.a. LwF) on Places2 -> VOC
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1,2,4],[1],[2,8],[1],[1],[2],[ 2 ],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case 'BRANCHMIT' 
        % different # of task-specific layers. Done for only _lockkeep_TEMP2 (a.k.a. LwF) on ImageNet -> MIT
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1,2,4],[1],[2,8],[1],[1],[2],[ 3 ],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;

    case 'BRANCHCUB'
        % different # of task-specific layers. Done for only _lockkeep_TEMP2 (a.k.a. LwF) on ImageNet -> CUB
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1,2,4],[1],[2,8],[1],[1],[2],[ 5 ],[0],[1]); % _lockkeep_TEMP2 only
        dumproot = '../dump/CUB_gridsearch_hinge/softmax_adam';
        ncls = 200;

    case {'LOSS', 'EXPANDLOSS'}
        % different loss functions. Done on Places2 -> VOC
        % note that this uses a different look-up table for g.g3
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[1,2,3,4,5],[1],[1],[2],[2],[0],[1]); % keep response balance lambda
        % [g.g3] method used. See next section for the meaning of each field.
        methods = struct( ...
            'str', {'_lock', '_lockkeep', '_lockkeep_TEMP2', '_lockkeep_L1', '_lockkeep_L2'}, ...
            'keep_orig', {2,1,1,1,1}, ...
            'copy_over', {0,1,1,1,1}, ...
            'temperature', {0,1,2,1,1}, ...
            'change_struct',{1,1,1,1,1}, ...
            'keep_loss', {'', 'MI', 'MI', 'L1', 'L2'});
        dumproot = '../dump/Places2_grid_search_losses';
        ncls = 20;

        % different loss functions for network expansion. 
        if strcmp(mode, 'EXPANDLOSS')
            g.g1(:) = 4; % Defaults to modifying 3 fc layers instead
            for i=1:numel(methods), methods(i).str = strrep(methods(i).str, '_lock', '_expandlock'); methods(i).change_struct = 2; end
        end

    case {'CUB', 'CUBsmall'}
        % Main experiment Alex* -> CUB.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[5,6],[0],[1]);
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/CUB_gridsearch_hinge/softmax_adam';
        ncls = 200;

    case 'CUBL2KEEP2'
        % Preserving parameter values by L2 loss, Alex* -> CUB. 
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,13],[1],[1],[2],[5,6],[0],[1]);
        g.g6(:) = str2num(mode(end)); % lambda for the L2 loss. g.g6=2 means lambda=1.
        dumproot = ['../dump/Paper_all_grid_search_l2keep/' mode(end)];
        ncls = 200;

    case {'MIT67', 'MIT67small'}
        % Main experiment Alex* -> MIT67.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[3,4],[0],[1]);
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;

    case 'MIT67L2KEEP2' % {'MIT67L2KEEP1', 'MIT67L2KEEP2', 'MIT67L2KEEP3', 'MIT67L2KEEP4'}
        % Preserving parameter values by L2 loss, Alex* -> MIT67. Change the last character to change g.g6 (specifies lambda)
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,13],[1],[1],[2],[3,4],[0],[1]);
        g.g6(:) = str2num(mode(end));
        dumproot = ['../dump/Paper_all_grid_search_l2keep/' mode(end)];
        ncls = 67;

    case {'VOC', 'VOCsmall'}
        % Main experiment Alex* -> VOC.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[1,2],[0],[1]); % just ImNet_VOC.
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case 'VOCL2KEEP4'
        % Preserving parameter values by L2 loss, Alex* -> VOC. Using a heavier weight (lambda=10) for preservation.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,13],[1],[1],[2],[1,2],[0],[1]);
        g.g6(:) = str2num(mode(end)); % lambda for the L2 loss. g.g6=4 means lambda=10.
        dumproot = ['../dump/Paper_all_grid_search_l2keep/' mode(end)];
        ncls = 20;

    case {'MNIST', 'MNISTsmall'}
        % Main experiment AlexImageNet -> MNIST.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[10],[0],[1]); % From ImageNet only for now
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 10;



    case 'VGGMIT'
        % Main experiment VGGImageNet -> MIT67.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[8],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;

    case 'VGGCUB'
        % Main experiment VGGImageNet -> CUB.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[7],[0],[1]);
        dumproot = '../dump/CUB_gridsearch_hinge/softmax_adam';
        ncls = 200;

    case 'EXPANDVGGMIT'
        % Network expansion VGGImageNet -> MIT67.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([4],[1],[10,11,12],[1],[1],[2],[8],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;

    case {'EXPANDVOC', 'EXPANDVOCsmall'}
        % Network expansion Alex* -> VOC.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([4],[1],[10,11,12],[1],[1],[2],[1,2],[0],[1]);
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case {'EXPANDMIT', 'EXPANDMITsmall'}
        % Network expansion Alex* -> MIT67.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([4],[1],[10,11,12],[1],[1],[2],[3,4],[0],[1]);
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;

    case {'EXPANDCUB', 'EXPANDCUBsmall'}
        % Network expansion Alex* -> CUB.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([4],[1],[10,11,12],[1],[1],[2],[5,6],[0],[1]);
        if strendswith(mode, 'small'), g.g2(:) = 4; end; % one experiment with 10% data
        dumproot = '../dump/CUB_gridsearch_hinge/softmax_adam';
        ncls = 200;

    case 'EXPANDTRDATA'
        % Network expansion while varying the data subsampled for each new task. Done on Places2 -> VOC
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([4],[1,3,4,5],[10,11,12],[1],[1],[2],[2],[0],[1]);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;

    case 'ACCUMVOC'
        % accumulative adding chunks of a new task. Done for Fine-tuning '_nokeep' and LwF '_lockkeep_TEMP2' on Places2 -> VOC
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[1,8],[1],[1],[2],[2],[-1,1,-2,2,-3,3],[1]); % g.g8 is the accumulation progress; negative means locking the shared layers first
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;
        batches_for_accum = {[1 2 4 6 7 14], [3 8 10 12 13 15 17 19], [5 9 11 16 18 20]}; % splits: animal / transport / others

    case 'ACCUMMIT'
        % accumulative adding chunks of a new task. Done for Fine-tuning '_nokeep' and LwF '_lockkeep_TEMP2' on ImageNet -> MIT67
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[1,8],[1],[1],[2],[3],[-1,1,-2,2,-3,3],[1]); % just ImNet_MIT67. full: 1,8 / -1,1,-2,2,-3,3
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;
        batch_groups = [1 3 1 2 2 3 3 2 2 2 1 3 1 2 1 3 2 2 1 3 2 3 3 3 2 2 2 3 1 1 2 3 3 1 1 2 2 3 2 2 1 3 1 2 1 2 1 1 3 2 3 2 1 3 2 3 2 3 3 1 2 1 3 2 3 1 1];
        for i=1:3, batches_for_accum{i} = find(batch_groups==i); end % splits: small room / large room / huge space

    case 'ACCUMMITspecial'
        % For accumulative adding chunks of a new task, we compare to feature-extraction and joint training, performed on three chunks of the new task.
        % Done on ImageNet -> MIT67. Note that for ImageNet -> VOC, the results can be extracted from the main experiment (1 chunk)
        % because the images for each chunk are all the same.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[4,5],[1],[1],[2],[3],[-1,-2,-3],[1]); % feat-extract and joint training, locked
        gA = g;
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[4],[1],[1],[2],[3],[3],[1]); % joint training, shared params unlocked
        g = concat_g(gA, g);
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 67;
        batch_groups = [1 3 1 2 2 3 3 2 2 2 1 3 1 2 1 3 2 2 1 3 2 3 3 3 2 2 2 3 1 1 2 3 3 1 1 2 2 3 2 2 1 3 1 2 1 2 1 1 3 2 3 2 1 3 2 3 2 3 3 1 2 1 3 2 3 1 1];
        for i=1:3, batches_for_accum{i} = find(batch_groups==i); end % splits: small room / large room / huge space

    case 'SHAREDLOWLRtry'
        % reducing the learning rate (0.1x) for the shared parameters. Done on Alex* -> VOC.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,8],[1],[1],[2],[1,2],[0],[2]);
        dumproot = '../dump/Paper_shared_weights_low_lr';
        ncls = 20;
    case 'Places2VOCtrval'
        % train on train/val of VOC and test on test set. Done on AlexPlaces2 -> VOCtrval.
        [g.g1, g.g2, g.g3, g.g4, g.g5, g.g6, g.g7, g.g8, g.g9] = ndgrid([1],[1],[2,1,4,5,8],[1],[1],[2],[9],[0],[1]); % just Places2_VOC. no joint training
        dumproot = '../dump/Paper_all_grid_search';
        ncls = 20;
        keyboard;

    otherwise
        assert(false, 'Unrecognized mode string.');
end




% -------------------------------------------------------------------------
% Settings interpretation (part 1)
%
% This section interprets the selected grid point to concrete parameter values.
% -------------------------------------------------------------------------

% [g.g1] number of old fc layers to treat as task-specific, and number of layers to re-initialize
% use fork_redos for other methods, and add_layers only for _asfeat: how many 
% layers to add in addition to what's already there.
fork_redos = [1,1; 2,1; 3,3; 3,1]; add_layers = [1,0,2,3]; 

% [g.g2] amount of train/test set being sub-sampled. 
% for reporting validation/test accuracy, the whole test set is used. The test
% set subsampling is just for quick evaluation after each epoch.
train_tests = [1,.2; .5,.2; .3,.2; .1,.1; .03,.1; .01,.1];

% [g.g3] the methods being compared.
%
% the names of the methods is explained as follows:
% _lock: a preliminary stage where we lock the shared parameters and train the
%        newly initialized parameters. All methods (excluding fine-tuning) 
%        continues from here by unlocking the parameters.
% _nokeep: Fine-tuning
% _joint: Joint Training
% _asfeat_5lr: Feature Extraction (with 5x learning rate bec. smaller network)
% _lockkeep_TEMP2: Learning without Forgetting (LwF)
% _expandlock: a preliminary stage similar to _lock but with network expansion
%              The next method continues from here.
% _expandlockkeep_TEMP2: Learning without Forgetting but w/ network expansion
% _expandlocklonger: Feature Extraction but with network expansion
% _lockl2keepweights: use L2 penalty for the parameters' difference from orig.
% The other ones are not used in the paper.
%
% The fields:
% keep_orig: 0 for fine-tune (no keeping), 1 for LwF, 2 for freezing (feat. extraction)
%            3 for joint training, 4 for using L2 loss
% copy_over: which method's result does this method continue from.
% temperature: the temperature for knowledge distillation, if LwF is used.
% change_struct: changing network structure. 1 for tree-structure, 2 for network expansion
% keep_loss: MI for cross-entropy/knowledge distillation. L1 for L1, L2 for L2. 
if isempty(methods)
  methods = struct( ...
    'str', {'_nokeep', '_lock', '_keep', '_joint', '_asfeat_5lr', ...
            '_lockkeep', '_asfeatkeep_5lrfeat', '_lockkeep_TEMP2', '_asfeatkeep_5lrfeat_TEMP2', '_expandlock', ...
            '_expandlockkeep_TEMP2', '_expandlocklonger', '_lockl2keepweights'}, ...
    'keep_orig',    {0,2,1,3,2, 1,1,1,1,2,  1, 2, 4}, ...
    'copy_over',    {2,0,0,2,0, 2,5,2,5,0, 10,10, 2}, ...
    'temperature',  {0,0,1,0,0, 1,1,2,2,0,  2, 0, 0}, ...
    'change_struct',{1,1,1,1,1, 1,1,1,1,2,  2, 2, 1}, ...
    'keep_loss', {'MI'});
end; change_struct = {'fork', 'expand'};

% [g.g4] base learning rate (each task has a different multiplier; this is global)
lrs = [0.0002];

% [g.g5] param initialization Gaussian scale (in addition to using glorot...
% therefore smaller than this and varies by task) 
scales = [0.05];

% [g.g6] the lambda for the response preserving loss
keep_response_lambdas = [0.3, 1, 3, 10];

% [g.g7] old-task-new-task pairs.
% lr_multiplier: used to be multiplied with the base learning rate
% epoch_multiplier: There are a few lines that uses this value to get the #epoch used to train -- sometimes not simply multipling.
% VOCtrval: train on the train+val set and evaluate on the test set of VOC.
transfer_scenarios = struct(...
    'orignet', {'Alex_ImNet', 'Alex_Places2', 'Alex_ImNet', 'Alex_Places2', 'Alex_ImNet', 'Alex_Places2', 'VGG_ImNet', 'VGG_ImNet', 'Alex_Places2', 'Alex_ImNet', 'Alex_Places2'}, ...
    'epoch_multiplier', {1, 1.5,    4, 6,   4, 6,   4, 4,   1.5,        2, 3  }, ...
    'lr_multiplier',    {1, 1  ,    2, 3,   4, 6,   4, 2,   1  ,        2, 2  }, ...
    'newtask', {'VOC', 'VOC',   'MIT67', 'MIT67',   'CUB', 'CUB',   'CUB', 'MIT67', 'VOCtrval', 'MNIST', 'MNIST'}); %orignet%

% [g.g8] cumulative adding new task. 0 for not cumulative, -1 for adding the
% first chunk but locking the shared parameters, 1 for unlocking and train,
% -2 for adding the second chunk but locking, 2 for unlock+train, and so on.
for_accum = 'hard coded; see g.g8 itself';

% [g.g9] the learning rate multiplier for the shared parameters.
sharedWeightLRmultipliers = [1, 0.1];


% finally generating the grid
grids = [g.g1(:), g.g2(:), g.g3(:), g.g4(:), g.g5(:), g.g6(:), g.g7(:), g.g8(:), g.g9(:)]

% put different iruns into different folders
if opts.irun~=1
    dumproot = sprintf('%s_irun%d', dumproot, opts.irun);
end

% -------------------------------------------------------------------------
% Looping through grid points
%
% This section further interprets the selected grid point and evaluate them
% -------------------------------------------------------------------------

errors = -ones(4, size(grids, 1));
aps = zeros(ncls, size(grids, 1));
starttimestr = datestr(datetime('now'));
flag_somethingnew = false; % bookkeeping for whether all results are obtained from cache

% loop through experiment grid points
for igrid = 1:numel(g.g1);

    gval = grids(igrid,:); % grid point

    fork = fork_redos(gval(1),1); % the number of old fc layers that are treated as task-specific
    redo = fork_redos(gval(1),2); % the number of new-task fc layers that are re-initialized randomly
    train_test_partial = train_tests(gval(2),:); % data subsampling
    method = methods(gval(3)); % method used
    scale = scales(gval(5)); % scale of Gaussian param initialization (on top of glorot)
    keep_response_lambda = keep_response_lambdas(gval(6)); % the lambda for the response preserving loss
    scenario = transfer_scenarios(gval(7)); % old-task-new-task pairs
    accum_mode = gval(8); % mode/stage for cumulative adding new task
    sharedWeightLRmultiplier = sharedWeightLRmultipliers(gval(9)); % the learning rate multiplier for the shared parameters

    % figuring out the learning rate
    lr = lrs(gval(4)) * scenario.lr_multiplier;

    % figuring out the total # of epochs that is used... for normal cases
    epochs = min( ceil(8+10/train_test_partial(1)), 50 ) * ((redo+1)/2) * scenario.epoch_multiplier;
    str_nokeep = method.str; % the codename of method to use
    flag_copy_over = method.copy_over; % the method to continue from

    % specify where the results are dumped.
    dumpdir = sprintf('%s/%s_%s_fork_x_redo_x_partial_x%s/fork_%d_redo_%d_partial_tr_%.2f', dumproot, scenario.orignet, scenario.newtask, str_nokeep, fork, redo, train_test_partial(1));
    % There will be a different format for fine-tuning (bec. difference in layer treatment)
    % and cumulative new task (bec. difference in dataset handling)

    % special treatment for _asfeat (fine-tuning) / _asfeatkeep (not used) / etc
    origfc_adddropout = true; % normally dropout is added for the fc's...
    if strncmp(str_nokeep, '_asfeat',7)
        % longer # epochs for added layer
        epochs = epochs / (redo+1);
        add_layer = add_layers(gval(1));
        fork = 1; redo = 1;
        lr = lr;
        origfc_adddropout = false;  % as part of locking the shared layers, no dropout for them
        if strcmp(str_nokeep, '_asfeat_5lr'), lr = lr * 5; end; % for increasing the learning rate when fine-tuning
        epochs = (epochs * (redo+1+add_layer)); % longer # epochs for added layer
        % different dump folder name format
        dumpdir = sprintf('%s/%s_%s_fork_x_redo_x_partial_x%s/forkredo_1_new_%d_partial_tr_%.2f', dumproot, scenario.orignet, scenario.newtask, str_nokeep, add_layer, train_test_partial(1));
    else
        add_layer = 0;
    end

    %
    % Determine if the network should continue from the training state / result of another method such as _lock.
    % note the implementational difference of (1) copying files and continuing from the last epoch, and (2) starting training from a specified network.
    %
    if accum_mode
        % cumulative adding new task.
        % for new task -- continue from previous task training (delete momentum, lr, etc), but lock shared layers
        % unlock train -- continue from this task's locked version
        lastdumpdir = sprintf('%s_Accum_%d', dumpdir, abs(accum_mode)-1);
        lastepochmultiplier = 2;
        % when doing feature extractor / joint training pretrain(lock), directly continue from locked training result.
        if strncmp(str_nokeep, '_asfeat',7) || method.keep_orig == 3
            lastdumpdir = [lastdumpdir 'Lock']; lastepochmultiplier = 1; 
            if strncmp(str_nokeep, '_asfeat',7), assert(accum_mode < 0); end
            if method.keep_orig == 3, assert(accum_mode<0 || accum_mode==numel(batches_for_accum)); end
        end

        % figure out dumpdir, and whether to copy training state over from this task's shared-layers-locked stage or not
        if accum_mode < 0 % lock mode
            flag_copy_over = false;
            dumpdir = sprintf('%s_Accum_%dLock', dumpdir, abs(accum_mode));
            method.keep_orig = 2;
        else
            % continue from Lock dir.
            flag_copy_over = true;
            copy_over_sourcedir = sprintf('%s_Accum_%dLock', dumpdir, abs(accum_mode));
            dumpdir = sprintf('%s_Accum_%d', dumpdir, abs(accum_mode));
        end
        % figure out where the training starts from (find last task's final result network, not this task's shared-layers-locked stage)
        if abs(accum_mode) == 1, accumulative = 'original'; else accumulative = fullfile(lastdumpdir, sprintf('net-epoch-%d.mat', ceil(epochs*lastepochmultiplier))); end
        accumu_batches = batches_for_accum;
    else 
        % normal, not accumulative
        % continue from training state of shared-layers-locked stage?
        if flag_copy_over
            str_nokeep_source = methods(method.copy_over).str;
            copy_over_sourcedir = sprintf('%s/%s_%s_fork_x_redo_x_partial_x%s/fork_%d_redo_%d_partial_tr_%.2f', dumproot, scenario.orignet, scenario.newtask, str_nokeep_source, fork, redo, train_test_partial(1));
            if strncmp(str_nokeep, '_asfeat',7)
                % because they have different dumpdir format
                copy_over_sourcedir = sprintf('%s/%s_%s_fork_x_redo_x_partial_x%s/forkredo_1_new_%d_partial_tr_%.2f', dumproot, scenario.orignet, scenario.newtask, str_nokeep_source, add_layer, train_test_partial(1));
            end
        end
        accumulative = [];
        accumu_batches = {};
    end

    % Shape of learning rate schedule: 0.1x after 75%. 
    lr_structure = [1 1 1 .1];
    if flag_copy_over
        if sharedWeightLRmultiplier ~= 1
            % for reduced shared layer learning rate, the second stage is longer.
            epochs = epochs * 4; lr_structure = [lr_structure reshape(repmat(lr_structure,3,1), 1, []) ]; 
        else
            % If continue from some previous training, then the schedule is duplicated, 
            % but note that only the second half is used because the first half was done.
            epochs = epochs * 2; lr_structure = [lr_structure lr_structure]; 
        end
    end % assert(method.keep_orig==1);


    % start evaluating the grid point
    chkmkdir(dumpdir);
    infofile = fullfile(dumpdir, 'info.mat');

    if exist(infofile, 'file')
        % load from cache
        fprintf('Loading from cached: %s (%d epochs)\n', infofile, epochs);
        info = load(infofile);
        info = info.info;
    else
        % evaluate anew
        fprintf('Working on: %s (%d epochs)\n', infofile, epochs);
        if flag_generate_resultmat 
            % skip evaluations without cache and quickly generate the errors, aps matrices for the cached.
            % use NaN for those skipped.
            info.valall_ImNet.val.error = [nan;nan]; info.valall_NewTask.val.error = [nan;nan];
            info.valall_NewTask.ap = nan;
        else
            if flag_copy_over
                % copy over from some intermediate stage and continue from there
                % note the implementational difference of continuing from an epoch and starting training from a specified network.
                fprintf('Copying from %s...\nTo %s...\n', copy_over_sourcedir, dumpdir);
                status = copyfile(fullfile(copy_over_sourcedir, 'net-*.mat'), dumpdir);
                respfile = fullfile(copy_over_sourcedir, 'oldtask_response.mat');
                if exist(respfile, 'file')
                    status = copyfile(respfile, dumpdir);
                end
                % note that the training state (momentum, etc.) is used.
            end

            % the main function that we call. Not exactly PASCAL, but general for all datasets.
            [ ~, info ] = PASCALkeeporig( ...
                'orignet', scenario.orignet, ...
                'newtask', scenario.newtask, ...
                'change_struct', change_struct{method.change_struct}, ...
                'origfc_adddropout', origfc_adddropout, ...
                'path_dump', dumpdir, ...
                'keep_orig', method.keep_orig, ... %KEEPORIG%
                'fakesave', fakesave, ...
                'fork_layers', fork, ...
                'redo_layers', redo, ...
                'new_layers', add_layer, ...
                'new_layers_scale', scale, ...
                'lr_structure', lr_structure, ...
                'keep_response_loss', method.keep_loss, ...
                'distillation_temp', method.temperature, ...
                'weightInitMethod', 'glorot', ...
                'learningRate', lr, ...
                'sharedWeightLRmultiplier', sharedWeightLRmultiplier, ...
                'numEpochs', ceil(epochs), ...
                'keep_response_lambda', keep_response_lambda, ...
                'accumulative', accumulative, ...
                'accumu_batches', accumu_batches, ...
                'partial_traintest', train_test_partial, ...
                'partial_randseed', partial_randseeds(opts.irun) );
            flag_somethingnew = true;
        end
    end
    % get evaluation results from old/new tasks; store in ERRORS/APS
    for f={'valall_NewTask', 'valall_PASCAL'}, fnewtask = f{1}; if isfield(info, fnewtask), break; end; end;
    for f={'valall_OrigTask', 'valall_ImNet'}, foldtask = f{1}; if isfield(info, foldtask), break; end; end;
    err = cellfun(@(X) X.error, [{info.(foldtask).val} {info.(fnewtask).val}], 'UniformOutput', false); err = cat(1,err{:});
    errors(1:numel(err),igrid) = err;
    if accum_mode
        aps(:,igrid) = nan; aps(accumu_batches{abs(accum_mode)}, igrid) = info.(fnewtask).ap;
        for itask = 1:abs(accum_mode)-1
            aps(accumu_batches{itask}, igrid) = info.(foldtask)(itask+1).ap;
        end
    else 
        aps(:,igrid) = info.(fnewtask).ap;
    end

    if strcmp(mode, 'Places2VOCtrval') % get Places2 test set accuracy for online evaluation
        testfile = fullfile(dumpdir, 'Places2_test_info.mat');
        if ~exist(testfile, 'file')
            net = load(fullfile(dumpdir, sprintf('net-epoch-%d.mat', ceil(epochs)))); net = net.net;
            [ info ] = evalNet( 'Places2_test', net, 1, 'getLastFCOutput', true );
            save(testfile, 'info');
        end
    end

end

% push notification
if ~flag_generate_resultmat && flag_somethingnew
    Pushme(sprintf('grdsrchPASCAL %s done', mode), sprintf('Started at: %s\ndumpdir == %s', starttimestr, dumproot));
end

% These are for plotting
if flag_generate_resultmat
    assert(all(g.g3(:) == g.g3(1)));
    sprintf('%s/fork_x_redo_x_partial_x%s/res.mat', dumproot, str_nokeep)
    save(sprintf('%s/fork_x_redo_x_partial_x%s/res.mat', dumproot, str_nokeep), 'aps', 'errors');
end
% 
if opts.pause_on_done
    keyboard;
end








%% concat_g: concatenate two grids
function [g] = concat_g(gA, gB)
    fA = fieldnames(gA);
    fB = fieldnames(gB);
    assert(isequal(fA,fB));
    for f = fA'
        f = f{1};
        g.(f) = [ gA.(f)(:); gB.(f)(:) ];
    end

%% strendswith: whether the string STR ends with ENDSTR
function [outputs] = strendswith(str, endstr)
    outputs = strncmp(str(end:-1:1), endstr(end:-1:1), numel(endstr));
